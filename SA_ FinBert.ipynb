{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ee1f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713330d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d1025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1b5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (1.23.4)\n",
      "Requirement already satisfied: boto3 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (1.26.21)\n",
      "Requirement already satisfied: requests in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (2.28.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (1.13.0)\n",
      "Requirement already satisfied: regex in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.4.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.21 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.29.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.21->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.21->boto3->pytorch-pretrained-bert) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset(Dataset):\n",
    "    def __init__(self, x_y_list, vocab_path, max_seq_length=256, vocab = 'base-cased', transform=None):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.x_y_list = x_y_list\n",
    "        self.vocab = vocab\n",
    "        if self.vocab == 'base-cased':\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False, do_basic_tokenize=True)\n",
    "        elif self.vocab == 'finance-cased':\n",
    "            self.tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = False, do_basic_tokenize = True)\n",
    "        elif self.vocab == 'base-uncased':\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True) \n",
    "        elif self.vocab == 'finance-uncased':\n",
    "            self.tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        tokenized_review = self.tokenizer.tokenize(self.x_y_list[0][index])\n",
    "        \n",
    "        if len(tokenized_review) > self.max_seq_length:\n",
    "            tokenized_review = tokenized_review[:self.max_seq_length]\n",
    "            \n",
    "        ids_review  = self.tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "        \n",
    "        mask_input = [1]*len(ids_review)\n",
    "        \n",
    "        padding = [0] * (self.max_seq_length - len(ids_review))\n",
    "        ids_review += padding\n",
    "        mask_input += padding\n",
    "        \n",
    "        input_type = [0]*self.max_seq_length  \n",
    "        \n",
    "        assert len(ids_review) == self.max_seq_length\n",
    "        assert len(mask_input) == self.max_seq_length\n",
    "        assert len(input_type) == self.max_seq_length \n",
    "        \n",
    "        ids_review = torch.tensor(ids_review)\n",
    "        mask_input =  torch.tensor(mask_input)\n",
    "        input_type = torch.tensor(input_type)\n",
    "        \n",
    "        sentiment = self.x_y_list[1][index] \n",
    "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
    "        \n",
    "        input_feature = {\"token_type_ids\": input_type, \"attention_mask\":mask_input, \"input_ids\":ids_review}\n",
    "        \n",
    "        return input_feature, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08ac274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(x_y_list):\n",
    "    dict_labels = {'positive': 0, 'neutral':1, 'negative':2}\n",
    "    x_y_list_transformed = [[item[0], dict_labels[item[1]]] for item in x_y_list]\n",
    "    X = np.asarray([item[0] for item in x_y_list_transformed])\n",
    "    y = np.asarray([item[1] for item in x_y_list_transformed])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39384ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def financialPhraseBankDataset(dir_):\n",
    "    fb_path = os.path.join(dir_, 'FinancialPhraseBank-v1.0')\n",
    "    data_50 = os.path.join(fb_path, 'Sentences_50Agree.txt')\n",
    "    sent_50 = []\n",
    "    rand_idx = 45\n",
    "    \n",
    "    with open(data_50, 'rb') as fi:\n",
    "        for l in fi:\n",
    "            l = l.decode('utf-8', 'replace')\n",
    "            sent_50.append(l.strip())\n",
    "    \n",
    "    x_y_list_50 = [sent.split(\"@\") for sent in sent_50]\n",
    "    x50, y50 = transform_labels(x_y_list_50)\n",
    "    \n",
    "    data = [x50, y50]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], test_size=0.1, random_state=rand_idx, stratify=data[1])\n",
    "\n",
    "    y_train = pd.get_dummies(y_train).values.tolist()\n",
    "    y_test = pd.get_dummies(y_test).values.tolist()\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "            \n",
    "    final_data = [X_train, X_test, y_train, y_test] \n",
    "     \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c0d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c968daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassification(nn.Module):\n",
    "   \n",
    "    def __init__(self, weight_path, num_labels=2, vocab=\"base-cased\"):\n",
    "        super(BertClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.vocab = vocab \n",
    "        if self.vocab == \"base-cased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=28996, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "        elif self.vocab == \"base-uncased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "            \n",
    "        elif self.vocab == \"finance-cased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=28573, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "        elif self.vocab ==\"finance-uncased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=30873, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal(self.classifier.weight)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, graphEmbeddings=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "       \n",
    "        logits = self.classifier(pooled_output)\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8488ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_opt():\n",
    "    def __init__(self, model):\n",
    "        super(dense_opt, self).__init__()\n",
    "        self.lrlast = .001\n",
    "        self.lrmain = .00001\n",
    "        self.optim = optim.Adam(\n",
    "        [ {\"params\":model.bert.parameters(),\"lr\": self.lrmain},\n",
    "          {\"params\":model.classifier.parameters(), \"lr\": self.lrlast},\n",
    "       ])\n",
    "    \n",
    "    def get_optim(self):\n",
    "        return self.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abcc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c16c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579a9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
    "num_labels= len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3196fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"finance-uncased\"\n",
    "vocab_path = 'analyst_tone/analyst_tone/vocab'\n",
    "pretrained_weights_path = \"analyst_tone/analyst_tone/pretrained_weights\" # this is pre-trained FinBERT weights\n",
    "fine_tuned_weight_path = \"analyst_tone/analyst_tone/fine_tuned.pth\"      # this is fine-tuned FinBERT weights\n",
    "max_seq_length=256\n",
    "#device='cuda:0'\n",
    "device='cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6d0d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g-lim\\AppData\\Local\\Temp\\ipykernel_20632\\709340429.py:25: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self.classifier.weight)\n"
     ]
    }
   ],
   "source": [
    "model = BertClassification(weight_path= pretrained_weights_path, num_labels=num_labels, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f75add9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(fine_tuned_weight_path, map_location='cuda:0'))\n",
    "model.load_state_dict(torch.load(fine_tuned_weight_path, map_location=torch.device('cpu')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e246972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "925ced70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"there is a shortage of capital, and we need extra financing\", \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ecc33e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a shortage of capital, and we need extra financing \n",
      "FinBERT predicted sentiment:  negative \n",
      "\n",
      "growth is strong and we have plenty of liquidity \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "there are doubts about our finances \n",
      "FinBERT predicted sentiment:  negative \n",
      "\n",
      "profits are flat \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for sent in sentences: \n",
    "    tokenized_sent = tokenizer.tokenize(sent)\n",
    "    if len(tokenized_sent) > max_seq_length:\n",
    "        tokenized_sent = tokenized_sent[:max_seq_length]\n",
    "    \n",
    "    ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "    mask_input = [1]*len(ids_review)        \n",
    "    padding = [0] * (max_seq_length - len(ids_review))\n",
    "    ids_review += padding\n",
    "    mask_input += padding\n",
    "    input_type = [0]*max_seq_length\n",
    "    \n",
    "    input_ids = torch.tensor(ids_review).to(device).reshape(-1, max_seq_length)\n",
    "    attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, max_seq_length)\n",
    "    token_type_ids = torch.tensor(input_type).to(device).reshape(-1, max_seq_length)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "       \n",
    "        print(sent, '\\nFinBERT predicted sentiment: ', labels[torch.argmax(outputs).item()], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "390201b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>twee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>mchoicebanyule</td>\n",
       "      <td>australias monetary policy setting has not bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>faezahideris</td>\n",
       "      <td>fiscal policy monetary policy and growth policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>MohrGarrett</td>\n",
       "      <td>understanding the money supply and monetary po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>tinybiscuitjr</td>\n",
       "      <td>images of sugar plum fairies and monetary poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>anish_sinha</td>\n",
       "      <td>wpi data rbis mid quarter monetary policy revi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date            User  \\\n",
       "0  12/16/2013  mchoicebanyule   \n",
       "1  12/16/2013    faezahideris   \n",
       "2  12/16/2013     MohrGarrett   \n",
       "3  12/16/2013   tinybiscuitjr   \n",
       "4  12/16/2013     anish_sinha   \n",
       "\n",
       "                                                twee  \n",
       "0  australias monetary policy setting has not bee...  \n",
       "1   fiscal policy monetary policy and growth policy   \n",
       "2  understanding the money supply and monetary po...  \n",
       "3  images of sugar plum fairies and monetary poli...  \n",
       "4  wpi data rbis mid quarter monetary policy revi...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = pd.read_csv('USmoneytary.csv')\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2f05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated tweets\n",
    "tweet.drop_duplicates(subset='twee',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbee1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = list(tweet['twee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7a65aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7665eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m headlines: \n\u001b[1;32m----> 5\u001b[0m     tokenized_sent \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokenized_sent) \u001b[38;5;241m>\u001b[39m max_seq_length:\n\u001b[0;32m      7\u001b[0m         tokenized_sent \u001b[38;5;241m=\u001b[39m tokenized_sent[:max_seq_length]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py:110\u001b[0m, in \u001b[0;36mBertTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    108\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordpiece_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(token):\n\u001b[0;32m    112\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(sub_token)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py:217\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124;03m\"\"\"Tokenizes a piece of text.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_chinese_chars(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py:307\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m    308\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xfffd\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_control(char):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for sent in headlines: \n",
    "    tokenized_sent = tokenizer.tokenize(sent)\n",
    "    if len(tokenized_sent) > max_seq_length:\n",
    "        tokenized_sent = tokenized_sent[:max_seq_length]\n",
    "    \n",
    "    ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "    mask_input = [1]*len(ids_review)        \n",
    "    padding = [0] * (max_seq_length - len(ids_review))\n",
    "    ids_review += padding\n",
    "    mask_input += padding\n",
    "    input_type = [0]*max_seq_length\n",
    "    \n",
    "    input_ids = torch.tensor(ids_review).to(device).reshape(-1, 256)\n",
    "    attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, 256)\n",
    "    token_type_ids = torch.tensor(input_type).to(device).reshape(-1, 256)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "        outputs = F.softmax(outputs,dim=1)\n",
    "        #print(sent, '\\nBERT predicted sentiment: ', labels[torch.argmax(outputs).item()], '\\n')\n",
    "        tweet.loc[tweet['twee'] == sent, 'sentiment'] = labels[torch.argmax(outputs).item()]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963af503",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6227a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.to_csv('USmonetary_finBert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b58d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
