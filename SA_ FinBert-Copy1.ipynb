{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ee1f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713330d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d1025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1b5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (1.23.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (1.13.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (4.64.1)\n",
      "Requirement already satisfied: regex in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
      "Requirement already satisfied: boto3 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (1.26.21)\n",
      "Requirement already satisfied: requests in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-pretrained-bert) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.4.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.21 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.29.21)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pytorch-pretrained-bert) (1.26.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.21->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\g-lim\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.21->boto3->pytorch-pretrained-bert) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset(Dataset):\n",
    "    def __init__(self, x_y_list, vocab_path, max_seq_length=256, vocab = 'base-cased', transform=None):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.x_y_list = x_y_list\n",
    "        self.vocab = vocab\n",
    "        if self.vocab == 'base-cased':\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False, do_basic_tokenize=True)\n",
    "        elif self.vocab == 'finance-cased':\n",
    "            self.tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = False, do_basic_tokenize = True)\n",
    "        elif self.vocab == 'base-uncased':\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True) \n",
    "        elif self.vocab == 'finance-uncased':\n",
    "            self.tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        tokenized_review = self.tokenizer.tokenize(self.x_y_list[0][index])\n",
    "        \n",
    "        if len(tokenized_review) > self.max_seq_length:\n",
    "            tokenized_review = tokenized_review[:self.max_seq_length]\n",
    "            \n",
    "        ids_review  = self.tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "        \n",
    "        mask_input = [1]*len(ids_review)\n",
    "        \n",
    "        padding = [0] * (self.max_seq_length - len(ids_review))\n",
    "        ids_review += padding\n",
    "        mask_input += padding\n",
    "        \n",
    "        input_type = [0]*self.max_seq_length  \n",
    "        \n",
    "        assert len(ids_review) == self.max_seq_length\n",
    "        assert len(mask_input) == self.max_seq_length\n",
    "        assert len(input_type) == self.max_seq_length \n",
    "        \n",
    "        ids_review = torch.tensor(ids_review)\n",
    "        mask_input =  torch.tensor(mask_input)\n",
    "        input_type = torch.tensor(input_type)\n",
    "        \n",
    "        sentiment = self.x_y_list[1][index] \n",
    "        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
    "        \n",
    "        input_feature = {\"token_type_ids\": input_type, \"attention_mask\":mask_input, \"input_ids\":ids_review}\n",
    "        \n",
    "        return input_feature, list_of_labels[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08ac274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(x_y_list):\n",
    "    dict_labels = {'neutral': 0, 'positive':1, 'negative':1}\n",
    "    x_y_list_transformed = [[item[0], dict_labels[item[1]]] for item in x_y_list]\n",
    "    X = np.asarray([item[0] for item in x_y_list_transformed])\n",
    "    y = np.asarray([item[1] for item in x_y_list_transformed])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39384ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def financialPhraseBankDataset(dir_):\n",
    "    fb_path = os.path.join(dir_, 'FinancialPhraseBank-v1.0')\n",
    "    data_50 = os.path.join(fb_path, 'Sentences_50Agree.txt')\n",
    "    sent_50 = []\n",
    "    rand_idx = 45\n",
    "    \n",
    "    with open(data_50, 'rb') as fi:\n",
    "        for l in fi:\n",
    "            l = l.decode('utf-8', 'replace')\n",
    "            sent_50.append(l.strip())\n",
    "    \n",
    "    x_y_list_50 = [sent.split(\"@\") for sent in sent_50]\n",
    "    x50, y50 = transform_labels(x_y_list_50)\n",
    "    \n",
    "    data = [x50, y50]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], test_size=0.1, random_state=rand_idx, stratify=data[1])\n",
    "\n",
    "    y_train = pd.get_dummies(y_train).values.tolist()\n",
    "    y_test = pd.get_dummies(y_test).values.tolist()\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "            \n",
    "    final_data = [X_train, X_test, y_train, y_test] \n",
    "     \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c0d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c968daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassification(nn.Module):\n",
    "   \n",
    "    def __init__(self, weight_path, num_labels=2, vocab=\"base-cased\"):\n",
    "        super(BertClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.vocab = vocab \n",
    "        if self.vocab == \"base-cased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=28996, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "        elif self.vocab == \"base-uncased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "            \n",
    "        elif self.vocab == \"finance-cased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=28573, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "        elif self.vocab ==\"finance-uncased\":\n",
    "            self.bert = BertModel.from_pretrained(weight_path)\n",
    "            self.config = BertConfig(vocab_size_or_config_json_file=30873, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal(self.classifier.weight)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, graphEmbeddings=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "       \n",
    "        logits = self.classifier(pooled_output)\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8488ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_opt():\n",
    "    def __init__(self, model):\n",
    "        super(dense_opt, self).__init__()\n",
    "        self.lrlast = .001\n",
    "        self.lrmain = .00001\n",
    "        self.optim = optim.Adam(\n",
    "        [ {\"params\":model.bert.parameters(),\"lr\": self.lrmain},\n",
    "          {\"params\":model.classifier.parameters(), \"lr\": self.lrlast},\n",
    "       ])\n",
    "    \n",
    "    def get_optim(self):\n",
    "        return self.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abcc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c16c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579a9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {2:'negative', 0:'neutral',1:'positive'}\n",
    "num_labels= len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3196fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"finance-uncased\"\n",
    "vocab_path = 'analyst_tone/analyst_tone/vocab'\n",
    "pretrained_weights_path = \"analyst_tone/analyst_tone/pretrained_weights\" # this is pre-trained FinBERT weights\n",
    "fine_tuned_weight_path = \"analyst_tone/analyst_tone/fine_tuned.pth\"      # this is fine-tuned FinBERT weights\n",
    "max_seq_length=256\n",
    "#device='cuda:0'\n",
    "device='cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c6d0d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\g-lim\\AppData\\Local\\Temp\\ipykernel_13216\\709340429.py:25: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self.classifier.weight)\n"
     ]
    }
   ],
   "source": [
    "model = BertClassification(weight_path= pretrained_weights_path, num_labels=num_labels, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f75add9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(fine_tuned_weight_path, map_location='cuda:0'))\n",
    "model.load_state_dict(torch.load(fine_tuned_weight_path, map_location=torch.device('cpu')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e246972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file = vocab_path, do_lower_case = True, do_basic_tokenize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "925ced70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"there is a shortage of capital, and we need extra financing\", \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8e1b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(sentences, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ecc33e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a shortage of capital, and we need extra financing \n",
      "FinBERT predicted sentiment:  negative \n",
      "\n",
      "growth is strong and we have plenty of liquidity \n",
      "FinBERT predicted sentiment:  positive \n",
      "\n",
      "there are doubts about our finances \n",
      "FinBERT predicted sentiment:  negative \n",
      "\n",
      "profits are flat \n",
      "FinBERT predicted sentiment:  neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch in train_loader:\n",
    "    for sent in batch: \n",
    "        tokenized_sent = tokenizer.tokenize(sent)\n",
    "        if len(tokenized_sent) > max_seq_length:\n",
    "            tokenized_sent = tokenized_sent[:max_seq_length]\n",
    "    \n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sent)\n",
    "        mask_input = [1]*len(ids_review)        \n",
    "        padding = [0] * (max_seq_length - len(ids_review))\n",
    "        ids_review += padding\n",
    "        mask_input += padding\n",
    "        input_type = [0]*max_seq_length\n",
    "    \n",
    "        input_ids = torch.tensor(ids_review).to(device).reshape(-1, max_seq_length)\n",
    "        attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, max_seq_length)\n",
    "        token_type_ids = torch.tensor(input_type).to(device).reshape(-1, max_seq_length)\n",
    "    \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "            outputs = F.softmax(outputs,dim=1)\n",
    "       \n",
    "            print(sent, '\\nFinBERT predicted sentiment: ', labels[torch.argmax(outputs).item()], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebbb6ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "390201b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>twee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>mchoicebanyule</td>\n",
       "      <td>australias monetary policy setting has not bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>faezahideris</td>\n",
       "      <td>fiscal policy monetary policy and growth policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>MohrGarrett</td>\n",
       "      <td>understanding the money supply and monetary po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>tinybiscuitjr</td>\n",
       "      <td>images of sugar plum fairies and monetary poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/16/2013</td>\n",
       "      <td>anish_sinha</td>\n",
       "      <td>wpi data rbis mid quarter monetary policy revi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date            User  \\\n",
       "0  12/16/2013  mchoicebanyule   \n",
       "1  12/16/2013    faezahideris   \n",
       "2  12/16/2013     MohrGarrett   \n",
       "3  12/16/2013   tinybiscuitjr   \n",
       "4  12/16/2013     anish_sinha   \n",
       "\n",
       "                                                twee  \n",
       "0  australias monetary policy setting has not bee...  \n",
       "1   fiscal policy monetary policy and growth policy   \n",
       "2  understanding the money supply and monetary po...  \n",
       "3  images of sugar plum fairies and monetary poli...  \n",
       "4  wpi data rbis mid quarter monetary policy revi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = pd.read_csv('USmoneytary.csv')\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet1 = pd.read_csv('TweetData1.csv')\n",
    "# tweet1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = [tweet1, tweet]\n",
    "# tweets = pd.concat(frames)\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbee1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = list(tweet['twee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05d6f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, batch_size=1000):\n",
    "    return (seq[pos:pos + batch_size] for pos in range(0, len(seq), batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7a65aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7665eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 579.1448094844818 seconds ---\n",
      "--- 1203.2038407325745 seconds ---\n",
      "--- 1839.0341725349426 seconds ---\n",
      "--- 2453.4733576774597 seconds ---\n",
      "--- 3066.9797444343567 seconds ---\n",
      "--- 3691.8646783828735 seconds ---\n",
      "--- 4302.720986127853 seconds ---\n",
      "--- 4912.917807817459 seconds ---\n",
      "--- 5528.8486161231995 seconds ---\n",
      "--- 6151.324815273285 seconds ---\n",
      "--- 6762.093503236771 seconds ---\n",
      "--- 7372.5984761714935 seconds ---\n",
      "--- 7982.934673309326 seconds ---\n",
      "--- 8591.759633302689 seconds ---\n",
      "--- 9206.238329410553 seconds ---\n",
      "--- 9816.925726413727 seconds ---\n",
      "--- 10424.156040668488 seconds ---\n",
      "--- 11039.217862129211 seconds ---\n",
      "--- 11647.294757604599 seconds ---\n",
      "--- 12254.151119470596 seconds ---\n",
      "--- 12864.356486082077 seconds ---\n",
      "--- 13474.437756061554 seconds ---\n",
      "--- 14084.383423089981 seconds ---\n",
      "--- 14693.219878435135 seconds ---\n",
      "--- 15301.923071146011 seconds ---\n",
      "--- 15924.570262908936 seconds ---\n",
      "--- 16537.458084583282 seconds ---\n",
      "--- 17152.103796482086 seconds ---\n",
      "--- 17768.49943780899 seconds ---\n",
      "--- 18383.74963593483 seconds ---\n",
      "--- 19001.003683805466 seconds ---\n",
      "--- 19616.606495141983 seconds ---\n",
      "--- 20236.187544107437 seconds ---\n",
      "--- 20850.093858718872 seconds ---\n",
      "--- 21464.4630484581 seconds ---\n",
      "--- 22193.74031352997 seconds ---\n",
      "--- 22922.205516576767 seconds ---\n",
      "--- 23536.6173286438 seconds ---\n",
      "--- 24151.341715574265 seconds ---\n",
      "--- 24761.277224302292 seconds ---\n",
      "--- 25372.33903861046 seconds ---\n",
      "--- 25981.39205956459 seconds ---\n",
      "--- 26591.344125032425 seconds ---\n",
      "--- 27203.806885242462 seconds ---\n",
      "--- 27819.626761198044 seconds ---\n",
      "--- 28428.836835622787 seconds ---\n",
      "--- 29042.623018026352 seconds ---\n",
      "--- 29652.7741189003 seconds ---\n",
      "--- 30294.59539246559 seconds ---\n",
      "--- 30978.506996154785 seconds ---\n",
      "--- 31637.754534244537 seconds ---\n",
      "--- 32308.008382320404 seconds ---\n",
      "--- 33083.50939369202 seconds ---\n",
      "--- 33857.5304543972 seconds ---\n",
      "--- 34624.79608488083 seconds ---\n",
      "--- 35398.93004608154 seconds ---\n",
      "--- 36174.87183713913 seconds ---\n",
      "--- 36974.79126048088 seconds ---\n",
      "--- 37740.69918990135 seconds ---\n",
      "--- 38409.98417830467 seconds ---\n",
      "--- 39068.1191072464 seconds ---\n",
      "--- 39725.19684553146 seconds ---\n",
      "--- 40378.752267837524 seconds ---\n",
      "--- 41032.41722226143 seconds ---\n",
      "--- 41687.31978344917 seconds ---\n",
      "--- 42328.11931300163 seconds ---\n",
      "--- 42952.70237827301 seconds ---\n",
      "--- 43596.416249513626 seconds ---\n",
      "--- 44639.81264448166 seconds ---\n",
      "--- 45837.860280275345 seconds ---\n",
      "--- 47004.95609164238 seconds ---\n",
      "--- 48208.09391403198 seconds ---\n",
      "--- 49431.44993329048 seconds ---\n",
      "--- 50617.48604464531 seconds ---\n",
      "--- 51256.170333623886 seconds ---\n",
      "--- 51983.97392630577 seconds ---\n",
      "--- 52784.97731971741 seconds ---\n",
      "--- 53449.133536577225 seconds ---\n",
      "--- 54109.81256604195 seconds ---\n",
      "--- 54829.893005371094 seconds ---\n",
      "--- 55528.71190214157 seconds ---\n",
      "--- 56200.79404711723 seconds ---\n",
      "--- 56853.58107495308 seconds ---\n",
      "--- 57507.83104109764 seconds ---\n",
      "--- 58163.456592559814 seconds ---\n",
      "--- 58816.14183473587 seconds ---\n",
      "--- 59478.33388376236 seconds ---\n",
      "--- 60130.35601234436 seconds ---\n",
      "--- 60793.90230464935 seconds ---\n",
      "--- 61446.44180107117 seconds ---\n",
      "--- 62096.87805700302 seconds ---\n",
      "--- 62747.83944487572 seconds ---\n",
      "--- 63399.547370910645 seconds ---\n",
      "--- 64051.363819122314 seconds ---\n",
      "--- 64701.17659497261 seconds ---\n",
      "--- 65350.70343375206 seconds ---\n",
      "--- 66000.38980865479 seconds ---\n",
      "--- 66671.08197450638 seconds ---\n",
      "--- 67321.52073144913 seconds ---\n",
      "--- 67967.19610261917 seconds ---\n",
      "--- 68616.79259991646 seconds ---\n",
      "--- 69265.37452507019 seconds ---\n",
      "--- 69912.12898278236 seconds ---\n",
      "--- 70558.88131690025 seconds ---\n",
      "--- 71203.15310835838 seconds ---\n",
      "--- 71848.13975024223 seconds ---\n",
      "--- 72494.69834566116 seconds ---\n",
      "--- 73143.75855588913 seconds ---\n",
      "--- 73792.79840016365 seconds ---\n",
      "--- 74458.0660700798 seconds ---\n",
      "--- 75252.03151106834 seconds ---\n",
      "--- 75938.87817287445 seconds ---\n",
      "--- 76604.14177894592 seconds ---\n",
      "--- 77260.64851212502 seconds ---\n",
      "--- 77909.47651600838 seconds ---\n",
      "--- 78560.35460400581 seconds ---\n",
      "--- 79214.73946213722 seconds ---\n",
      "--- 79870.5602324009 seconds ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence_batch \u001b[38;5;129;01min\u001b[39;00m chunker(\u001b[38;5;28mlist\u001b[39m(tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwee\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentence_batch:\n\u001b[1;32m----> 6\u001b[0m         tokenized_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokenized_sentences) \u001b[38;5;241m>\u001b[39m max_seq_length:\n\u001b[0;32m      8\u001b[0m             tokenized_sentences \u001b[38;5;241m=\u001b[39m tokenized_sentences[:max_seq_length]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py:110\u001b[0m, in \u001b[0;36mBertTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    108\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordpiece_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(token):\n\u001b[0;32m    112\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(sub_token)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py:217\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124;03m\"\"\"Tokenizes a piece of text.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_chinese_chars(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_pretrained_bert\\tokenization.py:307\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m    308\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xfffd\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_control(char):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for sentence_batch in chunker(list(tweet['twee'])):\n",
    "    for sent in sentence_batch:\n",
    "        tokenized_sentences = tokenizer.tokenize(sent)\n",
    "        if len(tokenized_sentences) > max_seq_length:\n",
    "            tokenized_sentences = tokenized_sentences[:max_seq_length]\n",
    "    \n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_sentences)\n",
    "        mask_input = [1]*len(ids_review)        \n",
    "        padding = [0] * (max_seq_length - len(ids_review))\n",
    "        ids_review += padding\n",
    "        mask_input += padding\n",
    "        input_type = [0]*max_seq_length\n",
    "    \n",
    "        input_ids = torch.tensor(ids_review).to(device).reshape(-1, max_seq_length)\n",
    "        attention_mask =  torch.tensor(mask_input).to(device).reshape(-1, max_seq_length)\n",
    "        token_type_ids = torch.tensor(input_type).to(device).reshape(-1, max_seq_length)\n",
    "    \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(input_ids, token_type_ids, attention_mask)\n",
    "            outputs = F.softmax(outputs,dim=1)\n",
    "            tweet.loc[tweet['twee'] == sent, 'sentiment'] = labels[torch.argmax(outputs).item()]\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963af503",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6227a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.to_csv('USmonetary_finBert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b58d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b17533",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:'neutral', 1:'positive',2:'negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45dce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b31229",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('Test.csv')\n",
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = list(tweet['twee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be331507",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_encoding = tokenizer(headlines, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        #item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = Dataset(headlines_encoding, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3060259",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e33036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "Loader = DataLoader(Dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5490c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15397c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model.eval()\n",
    "for batch in Loader:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    #labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    #tweet.loc[tweet['twee'] == sent, 'sentiment'] = labels[torch.argmax(outputs).item()]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd406d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
